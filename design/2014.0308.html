#!/usr/bin/perl
# 99aeabc9ec7fe80b1b39f5e53dc7e49e      <- self-modifying Perl magic
# state:  PTJwaln/KqmcMcpKbu8IC2Gx2RSXiU2+UJvVL50tAEc
# istate: LFWQomYA1er/ReQoYxkDUZpja2rpQ5AGlkKBBIjHAEY
# id:     33445092b373cac92261bffb76185c44

# This is a self-modifying Perl file. I'm sorry you're viewing the source (it's
# really gnarly). If you're curious what it's made of, I recommend reading
# http://github.com/spencertipping/writing-self-modifying-perl.
#
# If you got one of these from someone and don't know what to do with it, send
# it to spencer@spencertipping.com and I'll see if I can figure out what it
# does.

# For the benefit of HTML viewers (this is a hack):
# <div id='cover' style='position: absolute; left: 0; top: 0; width: 10000px; height: 10000px; background: white'></div>

$|++;

my %data;
my %transient;
my %externalized_functions;
my %datatypes;

my %locations;          # Maps eval-numbers to attribute names

my $global_data = join '', <DATA>;

sub meta::define_form {
  my ($namespace, $delegate) = @_;
  $datatypes{$namespace} = $delegate;
  *{"meta::${namespace}::implementation"} = $delegate;
  *{"meta::$namespace"} = sub {
    my ($name, $value, %options) = @_;
    chomp $value;
    $data{"${namespace}::$name"} = $value unless $options{no_binding};
    &$delegate($name, $value) unless $options{no_delegate}}}

sub meta::eval_in {
  my ($what, $where) = @_;

  # Obtain next eval-number and alias it to the designated location
  @locations{eval('__FILE__') =~ /\(eval (\d+)\)/} = ($where);

  my $result = eval $what;
  $@ =~ s/\(eval \d+\)/$where/ if $@;
  warn $@ if $@;
  $result}

meta::define_form 'meta', sub {
  my ($name, $value) = @_;
  meta::eval_in($value, "meta::$name")};
eval `/home/spencertipping/r/initiative/perl-objects/notes serialize -p`; die $@ if $@;
eval `object serialize -p`; die $@ if $@;
meta::cache('parent-identification', <<'__');
/home/spencertipping/r/initiative/perl-objects/notes a9e5975593ed5d90d943ad98405c71e5
object 99aeabc9ec7fe80b1b39f5e53dc7e49e
__
meta::cache('parent-state', <<'__');
99aeabc9ec7fe80b1b39f5e53dc7e49e 8o6O4hqEnGDxNC5FgK+q340aKWykAb+LRLAe0Fi9xMc
a9e5975593ed5d90d943ad98405c71e5 21rFW0cbMFtbY56uoN16l/ZlsJxAHKK431eTvQChPI4
__
meta::data('author', 'Spencer Tipping');
meta::data('default-action', 'shell');
meta::data('license', <<'__');
MIT License
Copyright (c) 2010 Spencer Tipping

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
__
meta::data('permanent-identity', '33445092b373cac92261bffb76185c44');
meta::data('save-indirect', '1');
meta::data('watching', '1');
meta::hook('before-shell::ad', 'ad(\'note::\');');
meta::hook('before-shell::default-note', 'create(\'note::default\');');
meta::note('abstract-rewriting', <<'__');
Abstract rewriting.
An attempt to address the shortcomings of hard-quoting by using soft-quoting
for lexical closures and deferring expansion for undefined variables. For
example, assuming {} is a soft-quote:

| $ def bar "hi there"
  $ def foo {echo $bar}
  $ foo
  hi there
  $ echo $foo
  echo "hi there"
  $

The problem here is that expansion can happen too soon, and in some cases it
will happen twice. For example:

| $ def bar "\$bif"
  $ def bif 10
  $ def foo {echo $bar}                 <- $bar becomes $bif immediately
  $ foo                                 <- evaluate "echo $bif"
  10
  $

Now arguably $bar would actually become \$bif, since $ is known to be an active
character. This gives us expansion idempotence, which may be the key to this
all working out. It also gives the language some nice structure:

| 1. Unbound variable names can be prefixed with $, notably within patterns. So
     destructuring is notationally awesome.
  2. Once a variable is bound, it becomes impossible to change or shadow
     through a destructuring bind. So mutability is a feature of the binding
     environment, but variables are explicitly rewriting instructions.

That seems about right. The only difficulty is how we handle stuff like
constant folding:

| $ def x 10
  $ def foo {echo $x}                   <- {echo 10}
  $ def foo {echo (echo $x)}            <- {echo 10}, since echo is pure
  $ def foo {echo (+ 1 $x)}             <- {echo 11}, since + is pure

I think it's fine to commit to some level of evaluation like this. It means we
can do some fun stuff like this:

| $ echo (+ $x $y)
  (+ $x $y)
  $ echo (+ $x 1 2 3)
  (+ $x 6)
  $ count $x
  (count $x)
  $ inc (count $x)
  (inc (count $x))
  $ let [$x [1 2]] inc (count $x)       <- let could be a regular function!
  3
  $

It's ok for 'let' to be a normal function because rewriting is associative and
idempotent. The outer layer of rewriting will fail to evaluate $x, so any
expressions involving $x will remain unevaluated but active. 'let' can then
fetch its arguments, apply the variables either as a binding-table update or by
manual rewriting, and then evaluate an expression. (Not terribly elegant or
fast, but feasible.)

This is cool; all we need now is to figure out what the scoping/rewriting API
looks like.

__
meta::note('default', <<'__');
Language trends in general.
So far xh doesn't really respect things going on with programming languages: it
is a throwback into Perl/C-style untypedness, it's ugly, and it's hopelessly
confusing. The equivalence between data and text is useful only for
serialization; if it's a core feature, we need to be doing more with it. It's
also no excuse to have confusing rewriting semantics.

Recent languages have tended to emphasize an orthogonal data structure model
(numbers, strings, lists, maps), localized mutability, and separation between
currently-running code and the data it modifies (i.e. there's not a lot of
evaling going on). Quoting tends to be avoided where possible; most languages
will introduce redundancies of all sorts to prevent programmers from having to
think about escaping things inside a quoted construct. It's like a modeless
language: everything is interpreted the same way regardless of where it is.

There are limits, of course. The whole point of macros is that they introduce
localized modality, and when that aligns with real-world intuition it can be
very useful. But the level of abstraction is often higher than the evaluator;
as a macro user you don't think about the low-level stuff being interpreted
differently (at least, in most cases). Macros tend to be understood as ways to
move code, not to transform it (e.g. Clojure's defn, delay, future, etc).

But perhaps more importantly, macros aren't considered to be a huge feature in
most cases. Even in Clojure they're not the main selling point. Homoiconicity
may be something of an anachronism (albeit a useful one). Clojure is so usable
because it's simple and it has a great standard library. It makes a number of
decisions for you that help you simplify your code:

| 1. Just write functions and globals; no objects.
  2. If you need a seq, use a vector; if you need a map, use a map.
  3. If you need to quote a symbol, use a keyword.
  4. You get to use the normal seq stuff for lazy/infinite lists.
  5. If you need mutable state, use an atom.
  6. If you need to refer to a global, use its var.

In its current form, xh measures up like this:

| 1. Just write functions and globals; no objects.
  2. If you need a seq, you have four seemingly equal options and it matters
     which you choose.
  3. If you need to quote a symbol, use a string because everything is a
     string. You're not supposed to need quoted symbols, I guess.
  4. Everything is trivially lazy.
  5. If you need mutable state, declare a gensym variable.
  6. If you need to refer to a global, use its name as a string.

Maybe the problem here is that xh is too unopinionated about the meaning of
strings. It's fine to say there's an equivalence between values and strings in
the first place, and it's awesome if this can include the interpreter state.
But there needs to be a layer of operation where you don't need to think about
the string form of your data structures. In other words, the semantic
interpretation of your strings shouldn't be a leaky abstraction.

Devil's advocate: data is ultimately going to be bytes one way or another, so
don't we want to define the interpreter/data structures as a compression
strategy? Also, what about preserving the string form to make it easier to
think about stuff like editors?

I suspect it's not a bad idea to assert an equivalence between data and the
string it came from. It could be really useful for stuff like parsing: every
byte of input is stored somewhere in the parse tree (some of it as metadata, I
guess), so you can trivially get your input back.

In that case, xh is really a language for defining topological mappings over
string data. This is a nice way to think about it because it means that you can
apply different levels of interpretation to a string without losing the other
information. This makes it easier for computations to be commutative
(specifically, it means that commutative functions stay that way even when
you'd otherwise lose information).

A completely different way to look at it.
Let's suppose a parser is really a partition over bytes. It groups
possibly-disjoint ranges of bytes together, sometimes specifying aliases (e.g.
for terminals). Then by definition it's invertible, and it might be as simple
as mapping into a list of (partition-id, start, length) tuples. Likewise, a
transformation over any piece of parsed data is as simple as specifying a new
substring.

Perhaps more usefully, the whole purpose of a parser is to rearrange bytes from
one form to another, and this sometimes includes adding/deleting/etc. As a
result, data structures can be specified in terms of the parsers that
bijectively map them to readable strings. Since parsers can be composed, data
structures become trivially lazy.

I guess the challenge is really to define everything in terms of parsers. There
is some risk that this will degenerate into a horrendous untyped mess, but it
also takes care of representational abstraction, instruction stream
compression, and some instances of the halting problem.

Some thoughts:
| 1. All "state updates" are going to be achieved through allocation; we have a
     copy-on-write strategy.
  2. Because all allocations are tied to specific parse states, it's easy to
     quickly "un-allocate" a bunch of memory when a parse tree fails.
  3. There's a case to be made for having the heap be a humongous virtual area,
     only part of which is mapped into real memory at any moment. Then we get
     trivially generational GC by locality, and time/space tradeoffs can be
     made by throwing away parser outputs.
  4. I'm not sure how mutable data structures will be defined, nor how POSIX
     calls are going to work.
  5. Multimethods are trivial, as they can be functions of nonterminals.
     Similarly, we can use the same statistical optimization methods against
     all parse states to do branch prediction and compression.

Examples.
It should be possible to define the factorial function (pseudocode):

| factorial '0'     -> '1'
            (int n) -> (mult n (factorial (dec n)))

How about a list reverse function:

| reverse (nil)                          -> (nil)
  reverse (cons (element x) (list rest)) -> (append (reverse rest) [x])

How about something to parse /etc/passwd:

| passwd ''                                     -> []
  passwd (str (passwd-line x) "\n" (passwd xs)) -> (append x xs)
  passwd-line (join ":" (field u)
                        (field x)
                        (int userid)
                        (int groupid)
                        (field groupname)
                        (field homedir)
                        (field shell)) -> [u x userid groupid groupname
                                           homedir shell]
  field -> #"[^:]*"

Ok, it seems like we're just defining a regular Lisp with aggressive
destructuring, invertible functions (when pure), and implicit control flow with
nested success and implied preferential disjunction. It's almost like a
language focused on transactions: either it succeeds and we get a result, or it
fails and we try the next option.

Possible syntax.
We still need prefix operators and ubiquitous quasiquoting, so how about this:

| echo hi                       # normal function call, args as lazy string _
  echo (echo hi)                # function substitution
  def foo hi
  echo $foo                     # variable substitution

Because substitution is fundamentally a string operation, we're asserting the
equivalence of everything to strings. However, it's worth specifying
substitution as a list-level idea that has a flat variation:

| echo $foo                     # echo gets one argument
  echo $@foo                    # echo gets zero or more arguments

What about [] vs {}?

| echo [1 2 3 4]                # list of four elements
  echo {1 2 3 4}                # map of two k/v pairs
  echo (get {1 2 3 4} 1)        # -> 2
  echo (get [1 2 3 4] 0)        # -> 1

List <-> map could just be a matter of interpolation:

| def m  {a b c d}
  def xs [1 2 3 4]
  echo [$@m]                    # [a b c d]
  echo {$@xs}                   # {1 2 3 4}

So it's a Lisp, really, since all semantics are determined by the list
interpretation of values. The main philosophical difference is that all lists
in xh originate from strings, so they keep track of brackets and whitespace.
That is, xh's list reader is lossless. Which is great, but it also means that
string-equivalence and list-equivalence aren't the same (kind of like Clojure's
metadata, so maybe not the end of the world).

I like the language at this point, but there are some problems:
| 1. It's an extra character if we want to block interpolation. This may
     actually not be an issue, but it does complicate the reader a bit.
  2. We have no literal syntax for anonymous functions.
  3. We still have no way to indicate that some interpolations should happen
     and others should be quoted, beyond manually quoting stuff.
  4. It isn't obvious that parsers are integral to this language anymore, and
     it's even less clear how to manage parser definitions in a
     single-namespace definition model.

Possible syntax'.
The syntax above mostly works, but let's reappropriate {} for hard quoting.
That gives us a simple way to write anonymous functions. So we have the
following:

| E[{x}]     = x
  E[[x]]     = [E[x]]
  E[(x ...)] = C[E[x], E[...]]          # this supports anon functions

The semantics are still defined in terms of lists, which I think is probably
fine because we have both $ and $@. Expansion, then, is:

| bash                                  xh
  $ echo hi                             $ echo hi
  $ echo $foo                           $ echo $@foo
  $ echo "$foo"                         $ echo $foo
  $ echo $foo$bar                       $ echo @(str $foo $bar)
  $ echo ${!foo}                        $ echo $@$foo
  $ echo $(bar)                         $ echo @(bar)
  $ echo "$(bar)"                       $ echo (bar)

That solves (1) and (2) above. (3) may be a matter of writing a macro to
implement lexical scoping. I have no idea what to do about (4).

Lexical scoping.
If the language is homoiconic, then there's no reason we couldn't just write a
macro to implement lexical scoping through hard-quoted regions. HOWEVER, that
seems misguided because we're compromising the integrity of the hard-quoting
construct. In fact, it seems like a terrible idea now that I think about it.

The real problem is that functions aren't strictly hard-quoted because we
expect closure variables to work. We could use soft-quoting for functions,
making the rule that any undefined variable expansion is represented by some
lazy quantity (or some constant-folded abstract expression?).

__
meta::note('parser', <<'__');
XH parser definitions.
A self-hosting parser to try out the language so far.

def xh-word  (match '\s*([^(){}\[\]"\'\s]+)')
def xh-words (zero-or-more xh-one)

def (xh-list $xs)   (str '(' (xh-words $xs) ')')
def (xh-vector $xs) (str '[' (xh-words $xs) ']')
def (xh-map $xs)    (str '{' (xh-words $xs) '}')

def xh-string  (match '"((?:\\.|[^"])*)"')
def xh-qstring (match '\'((?:\\.|[^\'])*)\'')

This seems all wrong. The whole point of parsers is just to have transactional
functions and backtracking, which itself hints at some kind of relational
evaluator ... so maybe I want to do it this way:

def (xh-list  $xs) (str "(" (xh-words $xs) ")")
def (xh-words $xs) (join "" (map xh-word $xs))
def (xh-word  $x)  (str (get $x pre-whitespace)
                        (get $x data)
                        (get $x post-whitespace))

Almost. But how do we indicate that pre-whitespace, data, and post-whitespace
each have a pattern? We probably need to treat the pattern as a compression
codec:

def (xh-word $x) (str (/'\s*'               (%x pre-whitespace))
                      (/'[^"\'\[\](){}\s]+' (%x data))
                      (/'\s*'               (%x post-whitespace)))

Cool so far, but let's refactor a bit so that words don't contain their
whitespace (which is lame):

def xh-word            /'[^"\'\[\](){}\s]+'
def xh-whitespace      /'\s+'
def (xh-words $xs $ws) (->> (map (or xh-word xh-thing) $xs)
                            (interleave (map xh-whitespace $ws))
                            (join ""))

The long way to write data structures:

def (xh-list $xs $ws) (str '(' (xh-words $xs $ws) ')')
def (xh-vec  $xs $ws) (str '[' (xh-words $xs $ws) ']')
def (xh-map  $xs $ws) (str '{' (xh-words $xs $ws) '}')

def (xh-thing {type       list
               xs         $xs
               whitespace $ws}) (xh-list $xs $ws)

def (xh-thing {type       vec
               xs         $xs
               whitespace $ws}) (xh-vec $xs $ws)

def (xh-thing {type       map
               xs         $xs
               whitespace $ws}) (xh-map $xs $ws)

A shorter way:

def matchers {"(" ")" "[" "]" "{" "}"}
def (xh-thing {type       $m
               xs         $xs
               whitespace $ws}) (str (/'[\[({]' $m)
                                     (xh-words $xs $ws)
                                     (%matchers $m)))

__
meta::note('scoping-api', <<'__');
Scoping API.
I think it should be possible to write 'def' as a regular function. At that
point the evaluator looks like this (xh syntax):

| second (reduce {[$scope $result] $statement | evaluate --with-scope $scope \
                                                         $statement}
                 [[] nil]
                 $statements)

This example raises the interesting issue that we can't use variable names to
refer to lambda arguments; doing so would mean we have shadowing, which
completely fails in a situation where rewriting happens outside-in. This
disrupts the otherwise simple rule that anonymous functions are just strings,
since now we need to get to the variables.

Actually, we have a bigger problem: any function call in any inner function
could happen before that function is called. For example:

| def f {
    def g {+ (arg 0) (arg 1)}           # this gets expanded with f's args
    g (inc (arg 0)) (inc (arg 1))
  }

If we have argument processing at all, it needs to be in the form of some kind
of quasi-syntax magic that isn't a regular function call. (Actually, that's
probably good from a usability perspective.) And as long as we're doing
destructuring arg-bindings, we might as well handle specialization and
alternatives too.

| def factorial {
    0  | 1
    $n | * $n (factorial (dec $n))
  }

So really, an anonymous function is a localized scope augmentation. That's kind
of cool. Let's write it this way:

| def factorial {
    0  | 1
    $n | * $n (factorial (dec $n))
  }

Another cool thing is that laziness and abstract interpretation can both be
implemented in terms of these rules. For example, suppose we load the contents
of a file into a variable called $s. We might not know all of $s, but we
probably do know (count $s); so (count $s) has a binding. Likewise, $s is
ultimately accessed with things like (count) and (subs) -- and those bindings
become available when the read requests are complete.

This is great because it means that lazy evaluation is first-class in the most
important way, and a lazy expression can be fully serialized, since each
intermediate stage is well-defined.

__
meta::parent('/home/spencertipping/r/initiative/perl-objects/notes', <<'__');
function::note    b96+lYUBUJIx1rD5Ay9018BBRP5h++6/90bEaOx7+IY
function::notes   zvS1hx63x9gpPdLMcF68585BXdVWdSlqkkssFxqXZU4
meta::type::note  4P8m6fY9qL46kdlhIixbEVXgoGPCbIPKW6THhUtZd/0
parent::object    Q4+42U+xCxGddLIZDnUc031ma2zMh4LJa3dm9v3H8IM
__
internal::main();
__DATA__
